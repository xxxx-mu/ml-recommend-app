{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_S1Fku5FKzP"
      },
      "source": [
        "## Setting up environment and importing libraries\n",
        "\n",
        "In this segment, we install the libraries required and set up the environment to train the models. Please choose a GPU runtime in the Google Colab setting. It is also recommended to mount your Google Drive to the notebook so that the static files just need to be downloaded once and can be reused should you need to restart your runtime."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "niZel911Veyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4Wav8jVbrFm"
      },
      "outputs": [],
      "source": [
        "# install required libraries\n",
        "!pip install transformers timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4FwkbZBbjz6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import AutoModel, AutoTokenizer, get_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from time import perf_counter\n",
        "from PIL import Image\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDvGnEo1T83y"
      },
      "source": [
        "Common configurations to be used throughout the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzeBWDaST83y"
      },
      "outputs": [],
      "source": [
        "# use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrgsfj80T83z"
      },
      "outputs": [],
      "source": [
        "# set random seeds for repeatability\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed_val):\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cch-NviMT83z"
      },
      "outputs": [],
      "source": [
        "seed_val = 0\n",
        "set_seed(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpdcTFbVGLPM"
      },
      "source": [
        "## Data loading and training parameters\n",
        "\n",
        "This segment downloads the data which we are going to use for the tutorial and defines the paths to read data from, as well as training parameters which we are going to use for all three models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS7cmvDVGWED"
      },
      "outputs": [],
      "source": [
        "# HOME_FOLDER = '/content/drive/MyDrive/KDD/' # if mounted\n",
        "HOME_FOLDER = '/content/KDD/' # if not mounted\n",
        "WEBVISION_DATA_FOLDER = HOME_FOLDER + 'webvision_data/'\n",
        "IMAGE_FOLDER = WEBVISION_DATA_FOLDER + 'images/'\n",
        "RESULTS_FOLDER = HOME_FOLDER + 'results/'\n",
        "TRAINED_MODELS_FOLDER = HOME_FOLDER + 'trained_models/'\n",
        "os.makedirs(RESULTS_FOLDER, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxEe1QbohJSD"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $WEBVISION_DATA_FOLDER\n",
        "!wget \"https://drive.google.com/uc?id=1r4aTTbLuYgGrgpZLOgUH9sQ33DBsbOFm&export=download\" -O $WEBVISION_DATA_FOLDER/data.zip\n",
        "!unzip $WEBVISION_DATA_FOLDER/data.zip -d $WEBVISION_DATA_FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxgXJR36GmLA"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(WEBVISION_DATA_FOLDER + 'train.csv')\n",
        "df_test = pd.read_csv(WEBVISION_DATA_FOLDER + 'test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MIzayVDT830"
      },
      "source": [
        "Exceute the cells below to see a random label, text, image triplet from the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fe2YryQT830"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_sample(row_num):\n",
        "    sample_row = df_train.iloc[row_num]\n",
        "    print('Index:', row_num)\n",
        "    print('Label:', sample_row['label'])\n",
        "    print('Text:', sample_row['text'])\n",
        "    image_path = IMAGE_FOLDER + sample_row['img_path']\n",
        "    im = Image.open(image_path)\n",
        "    plt.imshow(im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d0eMncvT830"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "show_sample(randint(0, len(df_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OC2T4igT830"
      },
      "source": [
        "We create the mapping table to map the string labels to integers to be used for the class labels and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OrZzFWzG1vS"
      },
      "outputs": [],
      "source": [
        "label_to_id = {lab:i for i, lab in enumerate(df_train['label'].sort_values().unique())}\n",
        "id_to_label = {v:k for k,v in label_to_id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O31tEGXpG5B8"
      },
      "outputs": [],
      "source": [
        "label_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfLhMctDG6xF"
      },
      "outputs": [],
      "source": [
        "num_out_labels = len(label_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNX_oH_HXQLm"
      },
      "outputs": [],
      "source": [
        "## training parameters to be used for all models ##\n",
        "num_train_epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 1.0e-5\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 0\n",
        "max_seq_length = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqdvy6HsH0mu"
      },
      "source": [
        "## BERT\n",
        "The first model which we are going to train is a BERT model which only uses the text from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0aX5qqnIEB1"
      },
      "source": [
        "### Dataset\n",
        "Since we are training a text only model, the dataset which we fit into the model only requires two attributes: **text** and **label**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOGfUvkeH9Wf"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, label_to_id, text_field=\"text\", label_field=\"label\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_to_id = label_to_id\n",
        "        self.text_field = text_field\n",
        "        self.label_field = label_field\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.df.at[index, self.text_field])\n",
        "        label = self.label_to_id[self.df.at[index, self.label_field]]\n",
        "\n",
        "        return text, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZVgNl7IHqP"
      },
      "source": [
        "### Model\n",
        "The model uses BERT to encode the text, and feeds the encodings (a 768 dimension vector) into a fully connected linear layer with 10 outputs (one for each class label).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1nlBu9P8saotjNg_nv_tfdnTxpxaFAhqq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQNZ2xACKl3y"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThlwRZ2DH7A_"
      },
      "outputs": [],
      "source": [
        "class VLBertModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_labels, text_pretrained='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_pretrained)\n",
        "        self.classifier = nn.Linear(\n",
        "            self.text_encoder.config.hidden_size, num_labels)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask, return_dict=True)\n",
        "        logits = self.classifier(output.last_hidden_state[:, 0, :]) # CLS embedding\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDOw8G2EI3Bv"
      },
      "outputs": [],
      "source": [
        "# create the model\n",
        "bert_model = VLBertModel(num_labels=num_out_labels, text_pretrained='bert-base-uncased')\n",
        "bert_model = bert_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzia70xiLLFi"
      },
      "source": [
        "### Training\n",
        "Load the data using the text dataset, feed it into a data loader for random sampling, and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWcI7V-hKKLp"
      },
      "outputs": [],
      "source": [
        "set_seed(seed_val)\n",
        "\n",
        "train_dataset = TextDataset(df=df_train, label_to_id=label_to_id, text_field='text', label_field='label')\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    sampler=train_sampler)\n",
        "\n",
        "\n",
        "t_total = len(train_dataloader) * num_train_epochs\n",
        "\n",
        "\n",
        "optimizer = AdamW(bert_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "bert_model.train()\n",
        "\n",
        "\n",
        "start = perf_counter()\n",
        "for epoch_num in trange(num_train_epochs, desc='Epochs'):\n",
        "    epoch_total_loss = 0\n",
        "\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc='Batch'):\n",
        "        b_text, b_labels = batch\n",
        "        b_inputs = bert_tokenizer(\n",
        "            list(b_text), truncation=True, max_length=max_seq_length,\n",
        "            return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "\n",
        "        b_labels = b_labels.to(device)\n",
        "        b_inputs = b_inputs.to(device)\n",
        "\n",
        "        bert_model.zero_grad()\n",
        "        b_logits = bert_model(text=b_inputs)\n",
        "\n",
        "        loss = criterion(b_logits, b_labels)\n",
        "\n",
        "        epoch_total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = epoch_total_loss/len(train_dataloader)\n",
        "\n",
        "\n",
        "    print('epoch =', epoch_num)\n",
        "    print('    epoch_loss =', epoch_total_loss)\n",
        "    print('    avg_epoch_loss =', avg_loss)\n",
        "    print('    learning rate =', optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "end = perf_counter()\n",
        "bert_training_time = end- start\n",
        "print('Training completed in ', bert_training_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WxtKGm3LQu_"
      },
      "source": [
        "### Testing\n",
        "Predict on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kFlKiEDLSrd"
      },
      "outputs": [],
      "source": [
        "bert_prediction_results = []\n",
        "\n",
        "test_dataset = TextDataset(df=df_test, label_to_id=label_to_id, text_field='text', label_field='label')\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            sampler=test_sampler)\n",
        "\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "  bert_model.eval()\n",
        "\n",
        "  b_text, b_labels = batch\n",
        "\n",
        "  b_inputs = bert_tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  b_labels = b_labels.to(device)\n",
        "  b_inputs = b_inputs.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      b_logits = bert_model(text=b_inputs)\n",
        "      b_logits = b_logits.detach().cpu()\n",
        "\n",
        "  bert_prediction_results += torch.argmax(b_logits, dim=-1).tolist()\n",
        "\n",
        "bert_prediction_labels = [id_to_label[p] for p in bert_prediction_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7zPgmnAT832"
      },
      "source": [
        "Generate the classification report by comparing the predictions from the model with the true labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VKAOiU1MC9c"
      },
      "outputs": [],
      "source": [
        "bert_class_report = classification_report(df_test['label'], bert_prediction_labels, output_dict=True)\n",
        "bert_class_report['training_time (seconds)'] = bert_training_time\n",
        "\n",
        "with open(RESULTS_FOLDER + 'bert_class_report.json', 'w') as f:\n",
        "  json.dump(bert_class_report, f)\n",
        "\n",
        "print(bert_class_report['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slmkgP1iT832"
      },
      "outputs": [],
      "source": [
        "# while True:pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiZKX8V6OUGB"
      },
      "source": [
        "## BERT + ResNet-50\n",
        "The next model that we are training uses a combination of BERT and ResNet-50 to encode the text and images, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B_AweX0Of5J"
      },
      "source": [
        "### Dataset\n",
        "Unlike the previous Dataset used for BERT, we include images in this dataset by reading the image files and applying a series of transformations to them so that they can fit into the ResNet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvfc8ug3noFk"
      },
      "outputs": [],
      "source": [
        "class ResNetDataset(Dataset):\n",
        "    def __init__(self, df, label_to_id, train=False, text_field=\"text\", label_field=\"label\", image_path_field=\"img_path\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_to_id = label_to_id\n",
        "        self.train = train\n",
        "        self.text_field = text_field\n",
        "        self.label_field = label_field\n",
        "        self.image_path_field = image_path_field\n",
        "\n",
        "        # ResNet-50 settings\n",
        "        self.img_size = 224\n",
        "        self.mean, self.std = (\n",
        "            0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n",
        "\n",
        "\n",
        "        self.train_transform_func = transforms.Compose(\n",
        "                [transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(self.mean, self.std)\n",
        "                    ])\n",
        "\n",
        "        self.eval_transform_func = transforms.Compose(\n",
        "                [transforms.Resize(256),\n",
        "                    transforms.CenterCrop(self.img_size),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(self.mean, self.std)\n",
        "                    ])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.df.at[index, self.text_field])\n",
        "        label = self.label_to_id[self.df.at[index, self.label_field]]\n",
        "        img_path = IMAGE_FOLDER + self.df.at[index, self.image_path_field]\n",
        "\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        if self.train:\n",
        "          img = self.train_transform_func(image)\n",
        "        else:\n",
        "          img = self.eval_transform_func(image)\n",
        "\n",
        "        return text, label, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoDHIiyxOjDf"
      },
      "source": [
        "### Model\n",
        "The original ResNet model consists of a fully connected layer with 1000 classes at the end, to show the scores of each image belonging to that class. However, our output classes are different and we want to use the image features before the fully connected layer instead of the 1000-class output probabilities. Therefore, we \"extract\" this model out of the original ResNet model architecture by leaving out the fully connected layer.\n",
        "\n",
        "\n",
        "After that, we pair the extracted ResNet model with a BERT model and add a 10-class linear layer on top of them, like we did for the previous BERT classifier.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1vFL3V1LdRlamLjkoI7ieoimxbwGnR7mU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJXrTaXHT832"
      },
      "source": [
        "The ResNet-50 model is trained on imagenet data to classify images into 1000 classes, therefore the last layer is a fully connected layer with 1000 output nodes. This output is not useful to us since our output classes are different. Therefore, we need to strip off this fully connected layer and use the features after the last average pooling layer. This can be done by copying the layers and weights to another network and leave out the last layer.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1ivYlubrhvY00P7b2SYLfpRSF3XxJUbfh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Svc1HgYPxW"
      },
      "outputs": [],
      "source": [
        "# extract layers of resnet-50 to build a new model\n",
        "\n",
        "import torch.nn as nn\n",
        "from torchvision.models.resnet import resnet50\n",
        "\n",
        "class ResNetFeatureModel(nn.Module):\n",
        "    def __init__(self, output_layer):\n",
        "        super().__init__()\n",
        "        self.output_layer = output_layer\n",
        "        pretrained_resnet = resnet50(pretrained=True)\n",
        "        self.children_list = []\n",
        "        for n,c in pretrained_resnet.named_children():\n",
        "            self.children_list.append(c)\n",
        "            if n == self.output_layer:\n",
        "                break\n",
        "\n",
        "        self.net = nn.Sequential(*self.children_list)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.net(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aSXjPsesrzn"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5GzS6y8pVYu"
      },
      "outputs": [],
      "source": [
        "# last output layer name for resnet is named 'layer4', dim 2048*7*7\n",
        "# last layer name before fc is named 'avgpool', dim 2048*1*1 -> needs to be flattened\n",
        "# reference: https://medium.com/the-owl/extracting-features-from-an-intermediate-layer-of-a-pretrained-model-in-pytorch-c00589bda32b\n",
        "\n",
        "class BertResNetModel(nn.Module):\n",
        "    def __init__(self, num_labels, text_pretrained='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_pretrained)\n",
        "        self.visual_encoder = ResNetFeatureModel(output_layer='avgpool')\n",
        "        self.image_hidden_size = 2048\n",
        "\n",
        "        self.classifier = nn.Linear(self.text_encoder.config.hidden_size + self.image_hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        text_output = self.text_encoder(**text)\n",
        "        text_feature = text_output.last_hidden_state[:, 0, :]\n",
        "        img_feature = self.visual_encoder(image)\n",
        "        features = torch.cat((text_feature, img_feature), 1)\n",
        "\n",
        "        logits = self.classifier(features)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnrdfEhhp3QX"
      },
      "outputs": [],
      "source": [
        "resnet_model = BertResNetModel(num_labels=num_out_labels, text_pretrained='bert-base-uncased')\n",
        "resnet_model = resnet_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbVr5GyFOmUe"
      },
      "source": [
        "### Training\n",
        "Similar to BERT training, but we take in images as an additional input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snzv98OZom95"
      },
      "outputs": [],
      "source": [
        "## training loop\n",
        "set_seed(seed_val)\n",
        "\n",
        "train_dataset = ResNetDataset(df=df_train, label_to_id=label_to_id, train=True, text_field='text', label_field='label', image_path_field='img_path')\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    sampler=train_sampler)\n",
        "\n",
        "\n",
        "t_total = len(train_dataloader) * num_train_epochs\n",
        "\n",
        "\n",
        "optimizer = AdamW(resnet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "resnet_model.train()\n",
        "\n",
        "start = perf_counter()\n",
        "for epoch_num in trange(num_train_epochs, desc='Epochs'):\n",
        "    epoch_total_loss = 0\n",
        "\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc='Batch'):\n",
        "        b_text, b_labels, b_imgs = batch\n",
        "        b_inputs = bert_tokenizer(\n",
        "            list(b_text), truncation=True, max_length=max_seq_length,\n",
        "            return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "\n",
        "        b_labels = b_labels.to(device)\n",
        "        b_imgs = b_imgs.to(device)\n",
        "        b_inputs = b_inputs.to(device)\n",
        "\n",
        "        resnet_model.zero_grad()\n",
        "        b_logits = resnet_model(text=b_inputs, image=b_imgs)\n",
        "\n",
        "        loss = criterion(b_logits, b_labels)\n",
        "\n",
        "        epoch_total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = epoch_total_loss/len(train_dataloader)\n",
        "\n",
        "\n",
        "    print('epoch =', epoch_num)\n",
        "    print('    epoch_loss =', epoch_total_loss)\n",
        "    print('    avg_epoch_loss =', avg_loss)\n",
        "    print('    learning rate =', optimizer.param_groups[0][\"lr\"])\n",
        "end = perf_counter()\n",
        "resnet_training_time = end- start\n",
        "print('Training completed in ', resnet_training_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXy-pTr1OqFX"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VPROY_Jr8AQ"
      },
      "outputs": [],
      "source": [
        "# testing loop\n",
        "\n",
        "resnet_prediction_results = []\n",
        "\n",
        "test_dataset = ResNetDataset(df=df_test, label_to_id=label_to_id, train=False, text_field='text', label_field='label', image_path_field='img_path')\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            sampler=test_sampler)\n",
        "\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "  resnet_model.eval()\n",
        "\n",
        "  b_text, b_labels, b_imgs = batch\n",
        "\n",
        "  b_inputs = bert_tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  b_labels = b_labels.to(device)\n",
        "  b_imgs = b_imgs.to(device)\n",
        "  b_inputs = b_inputs.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      b_logits = resnet_model(text=b_inputs, image=b_imgs)\n",
        "      b_logits = b_logits.detach().cpu()\n",
        "\n",
        "  resnet_prediction_results += torch.argmax(b_logits, dim=-1).tolist()\n",
        "\n",
        "resnet_prediction_labels = [id_to_label[p] for p in resnet_prediction_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMrldAOkT83-"
      },
      "source": [
        "Generate the classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIGirJocsLjv"
      },
      "outputs": [],
      "source": [
        "resnet_class_report = classification_report(df_test['label'], resnet_prediction_labels, output_dict=True)\n",
        "resnet_class_report['training_time (seconds)'] = resnet_training_time\n",
        "\n",
        "with open(RESULTS_FOLDER + 'resnet_class_report.json', 'w') as f:\n",
        "  json.dump(resnet_class_report, f)\n",
        "\n",
        "print(resnet_class_report['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WDJrB-ZT83-"
      },
      "outputs": [],
      "source": [
        "# while True:pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24cKM-7yFX3o"
      },
      "source": [
        "## ALBEF\n",
        "The last model that we are training is the ALBEF joint-encoder model which aligns the text and image features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xfvf5g9DJGn"
      },
      "source": [
        "### ALBEF-specific setup\n",
        "This section creates the folder structure and download the necessary files required to train an ALBEF model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZGS6gtILdzI"
      },
      "outputs": [],
      "source": [
        "ALBEF_FOLDER = HOME_FOLDER + 'ALBEF/'\n",
        "os.makedirs(ALBEF_FOLDER, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh3I4VxfDFQc"
      },
      "outputs": [],
      "source": [
        "# download pre-trained ALBEF model and required ALBEF files from ALBEF's official repo (only need to do this once to save it in your gdrive)\n",
        "!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/vit.py -O $ALBEF_FOLDER/vit.py\n",
        "!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/tokenization_bert.py -O $ALBEF_FOLDER/tokenization_bert.py\n",
        "!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/xbert.py -O $ALBEF_FOLDER/xbert.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gix7rI5Cjtv8"
      },
      "outputs": [],
      "source": [
        "# replace all occurrences of tokenizer_class with processor_class in xbert.py to make it compatible with newer transformers version\n",
        "# if you don't do this step, you will need to install transformers==4.8.1 as specified by the requirements in the ALBEF repo\n",
        "\n",
        "!sed -i 's/tokenizer_class/processor_class/g' $ALBEF_FOLDER/xbert.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogf5vTJ8E4jB"
      },
      "outputs": [],
      "source": [
        "# add path to downloaded ALBEF files\n",
        "import sys\n",
        "sys.path.append(ALBEF_FOLDER)\n",
        "\n",
        "#import libraries required for ALBEF\n",
        "from vit import VisionTransformer\n",
        "from xbert import BertConfig as AlbefBertConfig, BertModel as AlbefBertModel\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p675p6mOElsJ"
      },
      "source": [
        "### Dataset\n",
        "Same as the BERT-ResNet Dataset which contains **text**, **images** and **labels**. The only difference here is the image size (ResNet - 224, ALBEF - 256)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fceVnzgtffIa"
      },
      "outputs": [],
      "source": [
        "class AlbefDataset(Dataset):\n",
        "    def __init__(self, df, label_to_id, train=False, text_field=\"text\", label_field=\"label\", image_path_field=\"img_path\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_to_id = label_to_id\n",
        "        self.train = train\n",
        "        self.text_field = text_field\n",
        "        self.label_field = label_field\n",
        "        self.image_path_field = image_path_field\n",
        "\n",
        "        # ALBEF settings\n",
        "        self.img_size = 256\n",
        "        self.mean, self.std = (\n",
        "            0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n",
        "\n",
        "\n",
        "        self.train_transform_func = transforms.Compose(\n",
        "                [transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(self.mean, self.std)\n",
        "                    ])\n",
        "\n",
        "        self.eval_transform_func = transforms.Compose(\n",
        "                [transforms.Resize(256),\n",
        "                    transforms.CenterCrop(self.img_size),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(self.mean, self.std)\n",
        "                    ])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.df.at[index, self.text_field])\n",
        "        label = self.label_to_id[self.df.at[index, self.label_field]]\n",
        "        img_path = IMAGE_FOLDER + self.df.at[index, self.image_path_field]\n",
        "\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        if self.train:\n",
        "          img = self.train_transform_func(image)\n",
        "        else:\n",
        "          img = self.eval_transform_func(image)\n",
        "\n",
        "        return text, label, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfUMju-XFzgs"
      },
      "source": [
        "### Model\n",
        "ALBEF also uses BERT as its text encoder. Its image encoder is actually a VisionTransformer model.\n",
        "We use the joint text-image encoder to encode both the text and images, and as with the previous two models, add a linear fully connected layer to it.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1zcBBx08_7ujlH2RS2WZrmTZ--Icsk4NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avDhnSt_ogd1"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2wXdpo6bIIt"
      },
      "outputs": [],
      "source": [
        "class AlbefModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_config, num_labels, text_pretrained='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.text_encoder = AlbefBertModel.from_pretrained(\n",
        "            text_pretrained, config=bert_config, add_pooling_layer=False)\n",
        "\n",
        "        self.visual_encoder = VisionTransformer(\n",
        "            img_size=256, patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
        "            mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "        self.classifier = nn.Linear(\n",
        "            self.text_encoder.config.hidden_size, num_labels)\n",
        "\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        image_embeds = self.visual_encoder(image)\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image_embeds.device)\n",
        "        output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
        "                                   encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True\n",
        "                                   )\n",
        "        logits = self.classifier(output.last_hidden_state[:, 0, :])\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVpSMVmAT84A"
      },
      "source": [
        "Because ALBEF aligns the BERT and VisionTransformers features, it has its own BERT configuration. We download both this configuration and the pretrained model from Salesforce's GitHub and web pages in the function below which loads a pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAu-20KtT84A"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "def load_albef_pretrained(num_out_labels):\n",
        "    tmp_directory = './tmp/albef'\n",
        "    os.makedirs(tmp_directory, exist_ok=True)\n",
        "\n",
        "    albef_bert_config_fp = os.path.join(tmp_directory, 'config_bert.json')\n",
        "    albef_model_fp = os.path.join(tmp_directory, 'ALBEF.pth')\n",
        "\n",
        "    if not os.path.exists(albef_bert_config_fp):\n",
        "        urlretrieve(\"https://raw.githubusercontent.com/salesforce/ALBEF/main/configs/config_bert.json\", albef_bert_config_fp)\n",
        "\n",
        "    if not os.path.exists(albef_model_fp):\n",
        "        urlretrieve(\"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\", albef_model_fp)\n",
        "\n",
        "    albef_bert_config = AlbefBertConfig.from_json_file(albef_bert_config_fp)\n",
        "    albef_model = AlbefModel(bert_config=albef_bert_config, num_labels=num_out_labels)\n",
        "\n",
        "    albef_checkpoint = torch.load(albef_model_fp, map_location='cpu')\n",
        "    albef_state_dict = albef_checkpoint['model']\n",
        "\n",
        "    for key in list(albef_state_dict.keys()):\n",
        "        if 'bert' in key:\n",
        "            encoder_key = key.replace('bert.', '')\n",
        "            albef_state_dict[encoder_key] = albef_state_dict[key]\n",
        "            del albef_state_dict[key]\n",
        "\n",
        "    msg = albef_model.load_state_dict(albef_state_dict, strict=False)\n",
        "    print(\"ALBEF checkpoint loaded from \", albef_model_fp)\n",
        "    print(msg)\n",
        "    return albef_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gtT4vY2T84A"
      },
      "outputs": [],
      "source": [
        "albef_model = load_albef_pretrained(num_out_labels=num_out_labels)\n",
        "albef_model = albef_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lMzlczHF7vf"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i__SQkejJ1v"
      },
      "outputs": [],
      "source": [
        "## training loop\n",
        "set_seed(seed_val)\n",
        "\n",
        "train_dataset = AlbefDataset(df=df_train, label_to_id=label_to_id, train=True, text_field='text', label_field='label', image_path_field='img_path')\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    sampler=train_sampler)\n",
        "\n",
        "\n",
        "t_total = len(train_dataloader) * num_train_epochs\n",
        "\n",
        "\n",
        "optimizer = AdamW(albef_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "albef_model.train()\n",
        "\n",
        "start = perf_counter()\n",
        "for epoch_num in trange(num_train_epochs, desc='Epochs'):\n",
        "    epoch_total_loss = 0\n",
        "\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc='Batch'):\n",
        "        b_text, b_labels, b_imgs = batch\n",
        "        b_inputs = bert_tokenizer(\n",
        "            list(b_text), truncation=True, max_length=max_seq_length,\n",
        "            return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "\n",
        "        b_labels = b_labels.to(device)\n",
        "        b_imgs = b_imgs.to(device)\n",
        "        b_inputs = b_inputs.to(device)\n",
        "\n",
        "        albef_model.zero_grad()\n",
        "        b_logits = albef_model(text=b_inputs, image=b_imgs)\n",
        "\n",
        "        loss = criterion(b_logits, b_labels)\n",
        "\n",
        "        epoch_total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = epoch_total_loss/len(train_dataloader)\n",
        "\n",
        "\n",
        "    print('epoch =', epoch_num)\n",
        "    print('    epoch_loss =', epoch_total_loss)\n",
        "    print('    avg_epoch_loss =', avg_loss)\n",
        "    print('    learning rate =', optimizer.param_groups[0][\"lr\"])\n",
        "end = perf_counter()\n",
        "albef_training_time = end- start\n",
        "print('Training completed in ', albef_training_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGt9T1n2F-zm"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5_062z-TWJf"
      },
      "outputs": [],
      "source": [
        "# testing loop\n",
        "\n",
        "albef_prediction_results = []\n",
        "\n",
        "test_dataset = AlbefDataset(df=df_test, label_to_id=label_to_id, train=False, text_field='text', label_field='label', image_path_field='img_path')\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            sampler=test_sampler)\n",
        "\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "  albef_model.eval()\n",
        "\n",
        "  b_text, b_labels, b_imgs = batch\n",
        "\n",
        "  b_inputs = bert_tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  b_labels = b_labels.to(device)\n",
        "  b_imgs = b_imgs.to(device)\n",
        "  b_inputs = b_inputs.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      b_logits = albef_model(text=b_inputs, image=b_imgs)\n",
        "      b_logits = b_logits.detach().cpu()\n",
        "\n",
        "\n",
        "\n",
        "  albef_prediction_results += torch.argmax(b_logits, dim=-1).tolist()\n",
        "\n",
        "albef_prediction_labels = [id_to_label[p] for p in albef_prediction_results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rajcsz_WT84B"
      },
      "source": [
        "Generate the classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl-_hFYYZWk7"
      },
      "outputs": [],
      "source": [
        "albef_class_report = classification_report(df_test['label'], albef_prediction_labels, output_dict=True)\n",
        "albef_class_report['training_time (seconds)'] = albef_training_time\n",
        "\n",
        "with open(RESULTS_FOLDER + 'albef_class_report.json', 'w') as f:\n",
        "  json.dump(albef_class_report, f)\n",
        "\n",
        "print(albef_class_report['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFmus7PzUTtR"
      },
      "source": [
        "## Predict on models trained with 20 epochs\n",
        "In the previous segments, we trained each model for only 5 epochs due to the tutorial's time constraint. Thus, we cannot see a significant contrast between the accuracies of the models. Training for more epochs will improve the models' accuracies. Therefore, we have trained the models for 20 epochs each and saved them. In this segment, we will load the models and make predictions on the test set to compare their accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyu-SbQwT84B"
      },
      "source": [
        "The code in the previous segments have to be reused to load the models. Before executing this step, the following cells must have been executed:\n",
        "- Setup and common config cells\n",
        "- ALBEF-specific cells\n",
        "- ALBEF-loading cells\n",
        "- Cells containing model code for BERT, BERT-ResNet and ALBEF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79jsmG81T84B"
      },
      "outputs": [],
      "source": [
        "# Download trained_models.zip file to trained_models folder\n",
        "!wget \"https://drive.google.com/u/0/uc?id=1MGaKK4nHTd4FWDnvFihb6b8lASjeuN_l&export=download&confirm=t\" -O $HOME_FOLDER/trained_models.zip\n",
        "!unzip $HOME_FOLDER/trained_models.zip -d $HOME_FOLDER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBTbH0yjT84B"
      },
      "source": [
        "This function loads the pretrained model for each of the three model architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeNvqklrT84B"
      },
      "outputs": [],
      "source": [
        "def load_trained_models(load_directory, image_model_type):\n",
        "    label_map_filepath = os.path.join(load_directory, \"label_map.json\")\n",
        "    with open(label_map_filepath, 'r') as f:\n",
        "        label_to_id = json.load(f)\n",
        "\n",
        "    id_to_label = {v:k for k,v in label_to_id.items()}\n",
        "\n",
        "    num_labels = len(label_to_id)\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    model_sd_filepath = os.path.join(load_directory, \"state_dict.pt\")\n",
        "    model_sd = torch.load(model_sd_filepath, map_location='cpu')\n",
        "\n",
        "    if image_model_type is None:\n",
        "        model = VLBertModel(num_labels=num_labels)\n",
        "    elif image_model_type.lower() == 'resnet':\n",
        "        model = BertResNetModel(num_labels=num_labels)\n",
        "    elif image_model_type.lower() == 'albef':\n",
        "        model = load_albef_pretrained(num_out_labels=num_labels)\n",
        "\n",
        "    model.to('cpu') # load all models in cpu first\n",
        "    model.load_state_dict(model_sd, strict=True)\n",
        "    model.to(device)\n",
        "\n",
        "    return model, tokenizer, label_to_id, id_to_label\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDdjA1LeT84B"
      },
      "source": [
        "We streamline the three different datasets presented previously into one common VLDataset class which has **text**, **images** and **labels**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kx6gpy-T84B"
      },
      "outputs": [],
      "source": [
        "class VLDataset(Dataset):\n",
        "    def __init__(self, df, label_to_id, train=False, text_field=\"text\", label_field=\"label\", image_path_field=None, image_model_type=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_to_id = label_to_id\n",
        "        self.train = train\n",
        "        self.text_field = text_field\n",
        "        self.label_field = label_field\n",
        "        self.image_path_field = image_path_field\n",
        "        self.image_model_type = image_model_type\n",
        "\n",
        "        # text only dataset\n",
        "        if image_model_type is not None:\n",
        "\n",
        "            # ResNet-50 and ALBEF use different image sizes\n",
        "            if image_model_type.lower() == \"resnet\":   # ResNet-50 settings\n",
        "                self.img_size = 224\n",
        "            elif image_model_type.lower() == \"albef\":   # ALBEF settings\n",
        "                self.img_size = 256\n",
        "\n",
        "            self.mean, self.std = (\n",
        "                0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n",
        "\n",
        "\n",
        "            self.train_transform_func = transforms.Compose(\n",
        "                    [transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(self.mean, self.std)\n",
        "                        ])\n",
        "\n",
        "            self.eval_transform_func = transforms.Compose(\n",
        "                    [transforms.Resize(256),\n",
        "                        transforms.CenterCrop(self.img_size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(self.mean, self.std)\n",
        "                        ])\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.df.at[index, self.text_field])\n",
        "        label = self.label_to_id[self.df.at[index, self.label_field]]\n",
        "\n",
        "        # return images only if image model is specified\n",
        "        if self.image_model_type is not None:\n",
        "            img_path = IMAGE_FOLDER + self.df.at[index, self.image_path_field]\n",
        "\n",
        "\n",
        "            image = Image.open(img_path)\n",
        "            if self.train:\n",
        "                img = self.train_transform_func(image)\n",
        "            else:\n",
        "                img = self.eval_transform_func(image)\n",
        "\n",
        "            return text, label, img\n",
        "\n",
        "        else:\n",
        "            return text, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS8cRC-aT84C"
      },
      "source": [
        "We also streamline the predict function to do prediction on the test set with the loaded models of any of the three model architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj0OJKi5T84C"
      },
      "outputs": [],
      "source": [
        "## testing loop\n",
        "def predict(df_test, model, tokenizer, label_to_id, id_to_label, image_model_type):\n",
        "    prediction_results = []\n",
        "\n",
        "    test_dataset = VLDataset(df=df_test, label_to_id=label_to_id, train=False, text_field='text', label_field='label', image_path_field='img_path', image_model_type=image_model_type)\n",
        "    test_sampler = SequentialSampler(test_dataset)\n",
        "    test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                sampler=test_sampler)\n",
        "\n",
        "\n",
        "    for batch in tqdm(test_dataloader):\n",
        "        model.eval()\n",
        "\n",
        "        if image_model_type is None:\n",
        "          b_text, b_labels = batch\n",
        "          b_imgs = None\n",
        "        else:\n",
        "          b_text, b_labels, b_imgs = batch\n",
        "\n",
        "        b_inputs = tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        b_labels = b_labels.to(device)\n",
        "        b_inputs = b_inputs.to(device)\n",
        "\n",
        "        if b_imgs is not None:\n",
        "          b_imgs = b_imgs.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if b_imgs is not None:\n",
        "              b_logits = model(text=b_inputs, image=b_imgs)\n",
        "            else:\n",
        "              b_logits = model(text=b_inputs)\n",
        "\n",
        "            b_logits = b_logits.detach().cpu()\n",
        "\n",
        "\n",
        "\n",
        "        prediction_results += torch.argmax(b_logits, dim=-1).tolist()\n",
        "\n",
        "    prediction_labels = [id_to_label[p] for p in prediction_results]\n",
        "\n",
        "    print(accuracy_score(df_test['label'], prediction_labels))\n",
        "\n",
        "    return prediction_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaow05bgT84C"
      },
      "source": [
        "### Predict with loaded BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMZtqBO4T84C"
      },
      "outputs": [],
      "source": [
        "bert_load_directory = TRAINED_MODELS_FOLDER + 'BERT'\n",
        "bert_model, bert_tokenizer, label_to_id, id_to_label = load_trained_models(bert_load_directory, image_model_type=None)\n",
        "bert_predictions = predict(df_test.copy(), bert_model, bert_tokenizer, label_to_id, id_to_label, image_model_type=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BCr4ID6T84C"
      },
      "source": [
        "### Predict with loaded BERT-ResNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkDcrbHMT84C"
      },
      "outputs": [],
      "source": [
        "bert_resnet_load_directory = TRAINED_MODELS_FOLDER + 'BERT_ResNet'\n",
        "bert_resnet_model, bert_resnet_tokenizer, label_to_id, id_to_label = load_trained_models(bert_resnet_load_directory, image_model_type='resnet')\n",
        "bert_resnet_predictions = predict(df_test.copy(), bert_resnet_model, bert_resnet_tokenizer, label_to_id, id_to_label, image_model_type='resnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHp6QZ2AT84C"
      },
      "source": [
        "### Predict with loaded ALBEF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Elf576QZT84C"
      },
      "outputs": [],
      "source": [
        "albef_load_directory = TRAINED_MODELS_FOLDER + 'ALBEF'\n",
        "albef_model, albef_tokenizer, label_to_id, id_to_label = load_trained_models(albef_load_directory, image_model_type='albef')\n",
        "albef_predictions = predict(df_test.copy(), albef_model, albef_tokenizer, label_to_id, id_to_label, image_model_type='albef')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juLwvRHHT84C"
      },
      "source": [
        "### Save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHz5LZfpT84D"
      },
      "outputs": [],
      "source": [
        "df_out = df_test.copy()\n",
        "df_out['bert_predictions'] = bert_predictions\n",
        "df_out['bert_resnet_predictions'] = bert_resnet_predictions\n",
        "df_out['albef_predictions'] = albef_predictions\n",
        "df_out.to_csv(RESULTS_FOLDER + 'predictions_with_pretrained_models.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "f_S1Fku5FKzP",
        "m0aX5qqnIEB1",
        "p675p6mOElsJ",
        "5lMzlczHF7vf",
        "QGt9T1n2F-zm"
      ],
      "name": "multimodal_training.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}